{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab3\n",
    "\n",
    "Please run the code cells sequentially.\n"
   ],
   "id": "a32c30ea4230fd6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:34:18.198350Z",
     "start_time": "2025-06-04T05:34:17.342530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install the required packages\n",
    "! pip install -r requirements.txt"
   ],
   "id": "5b76e7658f5272cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker>=37.3.0 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (37.3.0)\n",
      "Requirement already satisfied: psycopg2>=2.9.10 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.9.10)\n",
      "Requirement already satisfied: pandas>=2.2.3 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.6.1 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
      "Requirement already satisfied: tzdata in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from faker>=37.3.0->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from pandas>=2.2.3->-r requirements.txt (line 3)) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from pandas>=2.2.3->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from pandas>=2.2.3->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from scikit-learn>=1.6.1->-r requirements.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from scikit-learn>=1.6.1->-r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from scikit-learn>=1.6.1->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\conestoga\\ai ml\\ml programming\\assignments\\lab 3\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->-r requirements.txt (line 3)) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:34:19.425521Z",
     "start_time": "2025-06-04T05:34:18.373197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "from utils.db_data_loader import DBDataLoader\n",
    "import tabulate as tb\n",
    "from utils.employee_model import EmployeeDataProcessor\n",
    "\n"
   ],
   "id": "91ade3b7fd3c1b41",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Database Creation\n",
    "The `employees` table is created in a PostgreSQL database using the following schema:\n",
    "- `employee_id`: Integer, primary key, auto-incremented\n",
    "- `name`: String\n",
    "- `position`: String (IT-related job titles)\n",
    "- `start_date`: Date (between 2015 and 2024)\n",
    "- `salary`: Integer ($60,000â€“$200,000)\n",
    "- `department_id`: Integer\n",
    "\n",
    "The `departments` table is created in a PostgreSQL database using the following schema:\n",
    "- `department_id`: Integer, primary key, auto-incremented\n",
    "- `department_name`: String\n",
    "- `location`: String\n",
    "- `budget`: Integer\n",
    "\n",
    "\n",
    "## Database Connection\n",
    "The connection string for the Neon.tech PostgreSQL database is stored in `utils/constants.py` as `CONNECTION_STRING`.\n",
    "\n",
    "## Fake Data Generation\n",
    "Fake employee data is generated using the `Faker` library in the `EmployeeSeeder` class (`utils/fake_data_creator.py`). It creates 100 records with realistic names, IT job titles, start dates, and salaries for the employee table and 10 rows for the department table.\n",
    "\n",
    "## Data Insertion\n",
    "To insert the generated data into the database, call the `generate_and_insert()` static method of the `EmployeeSeeder` class. This method connects to the database and populates the `employees` table with the fake data and populates department table with the corresponding departments of employees."
   ],
   "id": "e39315534fb54804"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:34:19.440714Z",
     "start_time": "2025-06-04T05:34:19.437576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Uncomment the 2 lines below to run the data insertion (100 records)\n",
    "# from utils.fake_data_creator import FakeDataSeeder\n",
    "# FakeDataSeeder.generate_and_insert()"
   ],
   "id": "4c967113d9af099f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Connect and Load Data\n",
    "\n",
    "`DBDataLoader`, a class to connect to the PostgreSQL database and load data from the employees and departments tables into Pandas DataFrames.\n",
    "\n",
    "Methods\n",
    "`__init__(self)`\n",
    "Initializes the database connection using the connection string from utils/constants.py.\n",
    "\n",
    "`load_employees(self)`\n",
    "Loads all records from the employees table into a Pandas DataFrame, closes the connection, and returns the DataFrame.\n",
    "\n",
    "`load_departments(self)`\n",
    "Loads all records from the departments table into a Pandas DataFrame, closes the connection, and returns the DataFrame.\n",
    "\n",
    "`close(self)`\n",
    "Closes the database connection."
   ],
   "id": "3d97ba9ba82e7c09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:34:20.730566Z",
     "start_time": "2025-06-04T05:34:19.459291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load employees and departments data into DataFrames\n",
    "employees_df = DBDataLoader().load_employees()\n",
    "departments_df = DBDataLoader().load_departments()\n",
    "\n",
    "#  Sorting employees by salary to show dirty records\n",
    "sorted_employee_df = employees_df.sort_values(by='salary', na_position='first')\n",
    "\n",
    "print(tb.tabulate(sorted_employee_df.head(),headers=\"keys\",tablefmt=\"outline\"))\n",
    "\n"
   ],
   "id": "b8d6bf7079570211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-----------------+--------------------+--------------+----------+-----------------+\n",
      "|    |   employee_id | name            | position           | start_date   |   salary |   department_id |\n",
      "+====+===============+=================+====================+==============+==========+=================+\n",
      "| 98 |           402 | Debra Davila    | Frontend Developer | 2023-02-18   |      nan |              74 |\n",
      "| 99 |           404 | Stephanie Giles | Software Engineer  | 2024-04-24   |      nan |              80 |\n",
      "|  1 |           403 | Alexandra Blake | QA Engineer        | 2016-04-17   |    60043 |              80 |\n",
      "| 51 |           454 | Hannah Ramirez  | Frontend Developer | 2021-05-17   |    61511 |              76 |\n",
      "| 30 |           433 | Jeffrey Johnson | Network Engineer   | 2020-02-12   |    61584 |              79 |\n",
      "+----+---------------+-----------------+--------------------+--------------+----------+-----------------+\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "1. Data Collection - The data was sourced from a PostgreSQL database hosted on Neon.tech. The employees and departments tables were populated with synthetic data using the Faker library via the FakeDataSeeder class in utils/fake_data_creator.py. Data was loaded into Pandas DataFrames using the DBDataLoader class.\n",
    "2. Data Cleaning - Duplicate or missing records removed, filled missing sallary fields with median. This is done using `remove_dupes_fill_salary` function in `EmployeeDataProcessor` class.\n",
    "3. Data Transformation - New columns were created for analysis. The start_date column was converted to a datetime type. Job titles were standardized to ensure consistency. These steps are done in `clean_and_process` method of `EmployeeDataProcessor` class.\n",
    "4. Feature Engineering - Additional features were derived to enhance analysis. For example, a years_of_service column was calculated by subtracting the start_date from the current date, providing insight into employee experience.\n",
    "5. Scaling - salary was scaled using normalization techniques (Min-Max scaling) to ensure comparability and to prepare the data for machine learning algorithms if needed. This is done in `scale_salary` method of `EmployeeDataProcessor` class.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9e04aaa34e96841b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the cleaned dataframe, you can see that 'years_of_service' is calculated, and the 'salary' column is normalized and added as a new column. Also the missing salary values are also now filled with median salary value.",
   "id": "8a20868fb92effe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T05:46:14.324517Z",
     "start_time": "2025-06-04T05:46:14.311542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "employee_data_processor = EmployeeDataProcessor(sorted_employee_df)\n",
    "employee_processed_df = employee_data_processor.clean_and_process()\n",
    "print(tb.tabulate(employee_processed_df.head(),headers=\"keys\",tablefmt=\"outline\"))\n"
   ],
   "id": "6ed5644370497531",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-----------------+--------------------+---------------------+----------+-----------------+--------------------+-----------------+\n",
      "|    |   employee_id | name            | position           | start_date          |   salary |   department_id |   years_of_service |   salary_scaled |\n",
      "+====+===============+=================+====================+=====================+==========+=================+====================+=================+\n",
      "| 98 |           402 | Debra Davila    | Frontend Developer | 2023-02-18 00:00:00 |   123692 |              74 |                  2 |       0.45715   |\n",
      "| 99 |           404 | Stephanie Giles | Software Engineer  | 2024-04-24 00:00:00 |   123692 |              80 |                  1 |       0.45715   |\n",
      "|  1 |           403 | Alexandra Blake | Qa Engineer        | 2016-04-17 00:00:00 |    60043 |              80 |                  9 |       0         |\n",
      "| 51 |           454 | Hannah Ramirez  | Frontend Developer | 2021-05-17 00:00:00 |    61511 |              76 |                  4 |       0.0105438 |\n",
      "| 30 |           433 | Jeffrey Johnson | Network Engineer   | 2020-02-12 00:00:00 |    61584 |              79 |                  5 |       0.0110681 |\n",
      "+----+---------------+-----------------+--------------------+---------------------+----------+-----------------+--------------------+-----------------+\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
